# Twibot-20 Tokenized Dataset (Parquet Format)

This directory contains the tokenized Twibot-20 dataset in Apache Parquet format, generated by `scripts/2_tokenize_dataset.py --use-parquet` or `scripts/convert_to_parquet.py`.

## Content

When generated, this directory will contain:
- `dataset_info.json`: Information about the dataset features and statistics
- `state.json`: Metadata about the dataset structure
- `train/`: Subdirectory containing the tokenized training split Parquet files
- `validation/`: Subdirectory containing the tokenized validation split Parquet files
- `test/`: Subdirectory containing the tokenized test split Parquet files

Each split subdirectory contains:
- `.parquet` files: Columnar storage format files containing the actual data

## Dataset Structure

The dataset contains all columns from the processed dataset, plus:
- `input_ids` (`Sequence(int32)`): List of token IDs
- `attention_mask` (`Sequence(int8)`): Mask indicating real tokens vs padding

## Advantages of Parquet Format

- **Storage Efficiency**: Up to 5.65x smaller than the Hugging Face format for tokenized data
- **Columnar Access**: Efficient for operations that only need specific columns
- **Compression**: Built-in compression reduces storage requirements
- **Interoperability**: Works well with other data processing tools like Pandas, Spark, etc.

## Note

The actual data files are not included in this repository due to their large size. Run `scripts/2_tokenize_dataset.py --use-parquet` to generate them.
