# Twibot-20 Processed Dataset (Hugging Face Format)

This directory contains the processed Twibot-20 dataset in Hugging Face format, generated by `scripts/1_fix_dataset.py`.

## Content

When generated, this directory will contain:
- `dataset_dict.json`: Metadata about the dataset structure
- `dataset_info.json`: Information about the dataset features and statistics
- `train/`: Subdirectory containing the training split (90% of original train)
- `validation/`: Subdirectory containing the validation split (10% of original train)
- `test/`: Subdirectory containing the test split

Each split subdirectory contains:
- `.arrow` files: Binary Arrow format files containing the actual data
- `.idx` files: Index files for efficient access

## Dataset Structure

The dataset contains the following columns:
- `user_id` (`string`): User identifier
- `text` (`string`): The combined, cleaned text from profile and tweets
- `features` (`string`): JSON string of the raw node data (for potential future use)
- `label` (`ClassLabel(names=['human', 'bot'])`): Integer label (0 or 1)

## Statistics

- Train: 7,450 samples (56.1% bots, 43.9% humans)
- Validation: 828 samples (56.0% bots, 44.0% humans)
- Test: 1,183 samples (54.1% bots, 45.9% humans)
- Average combined text length: ~150 characters per user

## Note

The actual data files are not included in this repository due to their large size. Run `scripts/1_fix_dataset.py` to generate them.
