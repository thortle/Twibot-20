# Twibot-20 Tokenized Dataset (Hugging Face Format)

This directory contains the tokenized Twibot-20 dataset in Hugging Face format, generated by `scripts/2_tokenize_dataset.py`.

## Content

When generated, this directory will contain:
- `dataset_dict.json`: Metadata about the dataset structure
- `dataset_info.json`: Information about the dataset features and statistics
- `train/`: Subdirectory containing the tokenized training split
- `validation/`: Subdirectory containing the tokenized validation split
- `test/`: Subdirectory containing the tokenized test split

Each split subdirectory contains:
- `.arrow` files: Binary Arrow format files containing the actual data
- `.idx` files: Index files for efficient access

## Dataset Structure

The dataset contains all columns from the processed dataset, plus:
- `input_ids` (`Sequence(int32)`): List of token IDs
- `attention_mask` (`Sequence(int8)`): Mask indicating real tokens vs padding

## Tokenization Details

- **Tokenizer:** `distilbert-base-uncased` from Hugging Face Transformers
- **Parameters:**
  - `truncation=True`: Sequences longer than the model's maximum input length (512 tokens for DistilBERT) are truncated
  - `padding=False`: Padding is applied dynamically per batch during training

## Statistics (Train Split)

- Average tokens per sample: ~41 tokens
- Maximum tokens in a sample: ~300 tokens
- Samples exceeding max length (truncated): < 1%
- Samples with essentially empty text (â‰¤ 2 tokens): ~2%

## Note

The actual data files are not included in this repository due to their large size. Run `scripts/2_tokenize_dataset.py` to generate them.
